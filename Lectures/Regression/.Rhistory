1-.63638
3.39/4.96655
1-.50376
10.06/4.966555
1 -.94253
1 - .94588
1*sqrt((36*37/12)*(2/9))
2*sqrt((36*37/12)*(2/9))
cMaxNorm( 0.0486,2,.5)
cMaxNor(0.0486,2,.5)
library(NSM3)
cMaxNorm( 0.0486,2,.5)
cMaxCorrNor(0.0486,2,.5)
sqrt((36(37)/12)*(2/6)) * 1.92
sqrt((36*(37)/12)*(2/6)) * 1.92
44/6
46/6
cMaxCorrNor(0.0102,2,.5)
sqrt((6*7)/12*(2/6))
sqrt((6*7*2/72))
sqrt((6*7*2/72))*cMaxCorrNor(0.0102,2,.5)
cMaxCorrNor(0.0102,2,.5)
35/6
cMaxCorrNor(0.06,3,.5)
sqrt((18*19*2/72))*cMaxCorrNor(0.0102,2,.5)
cMaxCorrNor(0.0102,2,.5)
d <- c(2.9,3.0,2.5,2.6,3.2,3.8,2.7,4.0,2.4,2.8,3.4,3.7,2.2,2.0)
g <- c(rep(1,5),rep(2,4),rep(3,5))
pNDWol(d,g)
d <- c(0,1,3,3,5,10,13,17,26,0,6,7,9,11,13,20,20,24,0,5,8,9,11,13,16,17,20,1,5,12,13,19,22,25,27,29)
g <- c(rep(1,9),rep(2,9),rep(3,9),rep(4,9))
jonckheere.test(d,g)
jonckheere.test(d,g,"increasing")
d <- c(1.53,1.71,1.91,2.08,2.44,1.6,2.04,2.36,2.37,2.45,2.52,2.73,1.66,2.10,2.78,2.88)g <- c(rep(1,5),rep(2,7),rep(1,5))
d <- c(1.53,1.71,1.91,2.08,2.44,1.6,2.04,2.36,2.37,2.45,2.52,2.73,1.66,2.10,2.78,2.88) g <- c(rep(1,5),rep(2,7),rep(1,5))
d <- c(1.53,1.71,1.91,2.08,2.44,1.6,2.04,2.36,2.37,2.45,2.52,2.73,1.66,2.10,2.78,2.88)
g <- c(rep(1,5),rep(2,7),rep(1,5))
jonckheere.test(d,g,"increasing")
g <- c(rep(1,5),rep(2,7),rep(1,4))
jonckheere.test(d,g,"increasing")
jonckheere.test(d,g,"decreasing")
jonckheere.test(d,g)
g <- c(rep(1,5),rep(2,7),rep(3,4))
jonckheere.test(d,g)
jonckheere.test(d,g,"increasing")
d<-c(2.5,2.6,2.9,3.0,3.2,2.4,2.7,3.8,4.0,2.0,2.2,2.8,3.4,3.7)
g<-c(rep(1,5),rep(2,4),rep(3,5))
pSDCFlig(d,g)
d <- c(0,1,3,3,5,10,13,17,26,0,6,7,9,11,13,20,20,24,0,5,8,9,11,13,16,17,20,1,5,12,13,19,22,25,27,29)
g <- c(rep(1,9),rep(2,9),rep(3,9),rep(4,9))
pSDCFlig(d,g)
a <- c(3.6,2.6,4.7,8,3.1,8.8,4.6,5.8,4.0,4.6)
b <- c(16.2,17.4,8.5,15.6,5.4,9.8,14.9,16.6,15.9,5.3,10.5)
sort(a)
sort(b)
sort(c(a,b))
length(a)
length(b)
10*11
a <- c(297,340,325,227,277,337,250,290)
b <- c(293,291,289,430,510,353,318)
sort(c(a,b))
sort(a)
sort(b)
a <- c(52.2,56.4,57.1,46.9,49.1,52.5,63,52,61.1,55.3,46.2,57.2)
b <- c(46.7,60.5,58.9,82.9,65.8,93.3,66.9,70.9,73.7,65.8,90.2,68.9)
length(a)
length(b)
d <- c(75.2,63.7,73.2,66.2,67.4,69.4,70.4,72.3,63.6,61.9,74.4,70.1)
length(d)
sort(a)
sort(b)
sort(d)
1+3+4+5+6+7+8+9+10+11+14+16
2+12+13+19.5+19.5+22+24+28+31+34+35+36
15+17+18+21+23+25+26+27+29+30
=23
15+17+18+21+23+25+26+27+29+30+32+33
94^2 + 276^2 + 296^2
172628/9
(172628/9)*(12/(36*37))
(172628/9)*(12/(36*37)) - 3(37)
(172628/9)*(12/(36*37)) - 3*(37)
data <- c(a,b,c)
groups <- c(rep(1,9),rep(2,9),rep(3,9))
kruskal.test(data,~groups)
?kruskal.test
kruskal.test(data~groups)
data
data <- c(a,b,d)
kruskal.test(data~groups)
data
length(d)
length(a)
groups <- c(rep(1,12),rep(2,12),rep(3,12))
94^2 + 276^2 + 296^2
172628/(36*37)
172628/(36*37) - (3*37)
kruskal.test(data~groups)
library(NSM3)
twin_1 <- c(277,169,157,139,108,213,232,229,114,232,161,149,128)
twin_2 <- c(256,118,137,144,146,221,184,188,97,231,114,187,230)
kendall.ci(twin_1,twin_2)
cor(twin_1,twin_2,method = "kendall",use = "pairwise")
23/(12*13/2)
cor.test(twin_1,twin_2, method = "kendall")
install.packages('Kendall')
library(Kendall)
Kendall(twin_1,twin_2)
12*13/2
25/(78)
26/(78)
27/(78)
for (i in seq(1,length(twin_1),1))
{i}
for (i in seq(1,length(twin_1),1))
{print(i)}
for (i in seq(1,length(twin_1),1)){}
for (i in seq(1,length(twin_1),1)){
for (j in seq(1,))
}
1>2
1>2 & 1>2
for (i in seq(1,length(twin_1),1)){
for
}
S <- 0
for (i in seq(1,length(twin_1),1)){
for (j in seq(i+1,length(twin_2),1)){
if ((twin_1[i] - twin_1[j])*(twin_2[i] - twin_2[j]) > 0 ){
S <- S+1
}else if ((twin_1[i] - twin_1[j])*(twin_2[i] - twin_2[j]) < 0 ){
S <- S-1
}
}
}
S
i
j
27/(13*12/2)
.348*76
.348*78
27/(78)
27/(79)
27/(77)
summary(Kendall(twin_1,twin_2))
for (i in seq(1,length(twin_1),1)){
for (j in seq(i+1,length(twin_2),1)){
print(i)
print(j)
print((twin_1[i] - twin_1[i])*(twin_1[j] - twin_2[j]))
}
}
for (i in seq(1,length(twin_1),1)){
for (j in seq(i+1,length(twin_2),1)){
print(i)
print(j)
print((twin_1[i] - twin_2[i])*(twin_1[j] - twin_2[j]))
print("BREAK")
}
}
for (i in seq(1,length(twin_1),1)){
for (j in seq(i+1,length(twin_2),1)){
print(i)
print(j)
print((twin_1[i] - twin_1[j])*(twin_2[i] - twin_2[j]))
print("BREAK")
}
}
52-23
52-25
12*13
12*13/2
53-25
12+6+5+6+6+4+2+4+4+3
5+5+3+2+3+3+1+2+!
12
5+5+3+2+3+3+1+2+1
52-25
52+25+1
27/sqrt(13*12*31/18)
pnorm(1.64724)
1-pnorm(1.64724)
1-pnorm(1.645)
1-pnorm(1.644)
1-pnorm(1.6445)
1-pnorm(1.6449)
1-pnorm(1.6448)
1-pnorm(1.64485)
1-pnorm(1.64489)
1-pnorm(1.64488)
1-pnorm(1.64486)
1-pnorm(1.64485)
1-pnorm(1.644855)
1-pnorm(1.644851)
1-pnorm(1.644852)
1-pnorm(1.644853)
1-pnorm(1.644854)
1-pnorm(1.6448535)
1-pnorm(1.6448536)
summary(Kendall(twin_1,twin_2))
?Kendall
qnorm(1.64724)
1-pnorm(1.64724)
years <- range(1950,1964,1)
years
years <- seq(1950,1964,1)
years
odor <- c(10,20,17,16,12,15,13,18,17,19,21,23,23,28,28)
for (i in seq(1,length(years),1)){
for (j in seq(i+1,length(years),1)){
print(years[j])
print(years[i])
print(odor[j] - odor[i])
print("BREAK")
}
}
years
length(years)
S <- 0
for (i in seq(1,length(years),1)){
for (j in seq(i+1,length(years),1)){
if (odor[j] - odor[i] > 0){
S <- S+1
}else if (odor[j] - odor[j] < 0){
S <- S-1
}
}
}
S
S <- 0
for (i in seq(1,length(years),1)){
for (j in seq(i+1,length(years),1)){
if (odor[j] - odor[i] > 0){
S <- S+1
}else if (odor[j] - odor[i] < 0){
S <- S-1
}
}
}
S
summary(Kendall(years,odor))
years <- seq(1950,1964,1)
odor <- c(10,20,17,16,12,15,13,18,17,19,21,23,23,28,28)
summary(Kendall(years,odor))
library(Kendall)
summary(Kendall(years,odor))
summary(Kendall(years,odor,"greater"))
cor.test(years,odor,"Kendall","greater")
?cor.test
cor.test(years,odor,method = "Kendall",alternative = "greater")
cor.test(years,odor,method = "kendall",alternative = "greater")
x <- seq(1,12,1)
x
y <- c(1,12,2,11,3,10,4,9,5,8,6,7)
length(y)
length(x)
cor.test(x,y,method = "kendall",alternative = "greater")
1/11
2*27/(13*12)
twin_1 <- c(277,169,157,139,108,213,232,229,114,232,161,149,128)
twin_2 <- c(256,118,137,144,146,221,184,188,97,231,114,187,230)
summary(Kendall(twin_1,twin_2))
2*27/(12*13)
twin_1 <- c(277,169,157,139,108,213,232,229,114,232,161,149,128)
twin_2 <- c(256,118,137,144,146,221,184,188,97,231,114,187,230)
library(Kendall)
summary(Kendall(twin_1,twin_2))
cor.test(twin_1,twin_2,method = "kendall",alternative = "greater")
?Kendall
.11202/2
years <- seq(1950,1964,1)
odor <- c(10,20,17,16,12,15,13,18,17,19,21,23,23,28,28)
Kendall(years,odor)
cor.test(years,odor,method = "kendall",alternative = "greater")
cFligPoli(.05,4,3)
library(NSM3)
cFligPoli(.05,4,3)
?cFligPoli(.101,8,7)
cFrd(.05,3,4)
1 + 1.5 + 1 + 2 + 1.5 + 1 + 1 + 1 + 1 + 2 + 1 + 2
2 + 1.5 + 2 + 1 + 1.5 + 2 + 2 + 2 + 2 + 1 + 2 + 1
16 + 2*20 + 3*36
12*(9.5^2 + 5^2 + 9.5^2) - 3*16*3*16
4*3*4 - 3
162/45
if(!require(psych)){install.packages("psych")}
if(!require(FSA)){install.packages("FSA")}
if(!require(lattice)){install.packages("lattice")}
if(!require(BSDA)){install.packages("BSDA")}
if(!require(multcompView)){install.packages("multcompView")}
if(!require(PMCMR)){install.packages("PMCMR")}
if(!require(rcompanion)){install.packages("rcompanion")}
if(!require(psych)){install.packages("psych")}
if(!require(FSA)){install.packages("FSA")}
if(!require(lattice)){install.packages("lattice")}
if(!require(BSDA)){install.packages("BSDA")}
if(!require(multcompView)){install.packages("multcompView")}
if(!require(PMCMR)){install.packages("PMCMR")}
if(!require(rcompanion)){install.packages("rcompanion")}
Input <- ("
Subject, point.one, point.six, one
'1', -.08, .01, .06
'2', .21, .17, .19
'3', .50, -.11, .34
'4', .14, .07, .14
")
Data <- read.table(textConnection(Input),header=TRUE)
Data
Input <- ("
Subject, point.one, point.six, one
'1' -.08 .01 .06
'2' .21 .17 .19
'3' .50 -.11 .34
'4' .14 .07 .14
")
Data <- read.table(textConnection(Input),header=TRUE)
Data
?friedman.test
Input <- ("
Subject, treatment, score
'1' 0.1 -.08
'1' 0.6 .01
'1' 1.0 .06
'2' 0.1 .21
'2' 0.6 .17
'2' 1.0 .19
'3' 0.1 .5
'3' 0.6 -.11
'3' 1.0 .34
'4' 0.1 .14
'4' 0.6 .07
'4' 1.0 .14
")
Data <- read.table(textConnection(Input),header=TRUE)
friedman.test(score ~ Subject | treatment, data = Data)
friedman.test(score ~ treatment | Subject, data = Data)
Input <- ("
Subject treatment score
'1' 0.1 -.08
'1' 0.6 .01
'1' 1.0 .06
'2' 0.1 .21
'2' 0.6 .17
'2' 1.0 .19
'3' 0.1 .5
'3' 0.6 -.11
'3' 1.0 .34
'4' 0.1 .14
'4' 0.6 .07
'4' 1.0 .14
")
Data <- read.table(textConnection(Input),header=TRUE)
friedman.test(score ~ treatment | Subject, data = Data)
friedman.test(score ~ Subject | treatment, data = Data)
2 + 2 + 1 + 3 + 3 + 1 + 3 + 3 + 2 + 2 + 2 + 2
3 + 3 + 3 + 2 + 2 + 2 + 2 + 2 + 3 + 3 + 1 + 3
1 + 1 + 2 + 1 + 1 + 3 + 1 + 1 + 1 + 1 + 3 + 1
3.314*3
1 - .9913
1 - .99113
2.6 * 18 * 3 * 4 /6
2.6 * sqrt(18 * 3 * 4 /6)
install.packages("geoR")
library(geoR)
library(geoR)
## Read in the coordinates defining the Galicia coastline, converting to the
## same spatial scale as the figure.
coastline <- read.table("galicia_coastline.txt", header=FALSE) / 100000
## Read in the coordinates defining the Galicia coastline, converting to the
## same spatial scale as the figure.
coastline <- read.table("galicia_coastline.txt", header=FALSE) / 100000
## Read in the coordinates defining the Galicia coastline, converting to the
## same spatial scale as the figure.
coastline <- read.table("galicia_coastline.txt", header=FALSE) / 100000
## Read in the coordinates defining the Galicia coastline, converting to the
## same spatial scale as the figure.
coastline <- read.table("/matthewosborns/documents/CourseWork/SpatialStats/galicia_coastline.txt", header=FALSE) / 100000
## Read in the coordinates defining the Galicia coastline, converting to the
## same spatial scale as the figure.
coastline <- read.table("/matthewosborne/documents/CourseWork/SpatialStats/galicia_coastline.txt", header=FALSE) / 100000
## Read in the coordinates defining the Galicia coastline, converting to the
## same spatial scale as the figure.
coastline <- read.table("/users/matthewosborne/documents/CourseWork/SpatialStats/galicia_coastline.txt", header=FALSE) / 100000
## Read in the coordinates defining the Galicia coastline, converting to the
## same spatial scale as the figure.
coastline <- read.table("galicia_coastline.txt", header=FALSE) / 100000
## Read in the coordinates defining the Galicia coastline, converting to the
## same spatial scale as the figure.
coastline <- read.table("galicia_coastline.txt", header=FALSE) / 100000
## Read in the lead values for the year 2000
df <- read.table(path + "galicia_lead_2000.txt", skip=2, header=TRUE)
path = "/Users/matthewosborne/Documents/Course Work/SpatialStats/"
## Read in the coordinates defining the Galicia coastline, converting to the
## same spatial scale as the figure.
coastline <- read.table("galicia_coastline.txt", header=FALSE) / 100000
## Read in the coordinates defining the Galicia coastline, converting to the
## same spatial scale as the figure.
coastline <- read.table(path + "galicia_coastline.txt", header=FALSE) / 100000
## Read in the coordinates defining the Galicia coastline, converting to the
## same spatial scale as the figure.
coastline <- read.table(("/Users/matthewosborne/Documents/Course Work/SpatialStats/galicia_coastline.txt", header=FALSE) / 100000
## Read in the coordinates defining the Galicia coastline, converting to the
## same spatial scale as the figure.
coastline <- read.table("/Users/matthewosborne/Documents/Course Work/SpatialStats/galicia_coastline.txt", header=FALSE) / 100000
## Read in the lead values for the year 2000
df <- read.table("/Users/matthewosborne/Documents/Course Work/SpatialStats/galicia_lead_2000.txt", skip=2, header=TRUE)
## Convert to the same spatial scale as the figure
df$x <- df$x / 100000
df$y <- df$y / 100000
## What are the names of the variables?
names(df)
## Extract the lead concentrations from the data frame 'df'
lead.conc <- df$z
lead.conc
## Calculate the log lead concentrations
log.lead.conc <- log(lead.conc)
round(log.lead.conc, 2)
## Produce a panel of plots, 4 rows high, by 2 columns across.
## Adjust the margins slightly for each plot.
par(mfrow=c(4,2), mar=c(4,4,1,1))
## Produce histograms or the lead concentrations on the original and
## log transformed scales
hist(lead.conc, xlab="Lead concentrations", main="")
hist(log.lead.conc, xlab="Log transformed lead concentration", main="")
## Are there any trends by x and y coordinates?
plot(df$x, lead.conc, xlab="x", ylab="Lead concentration")
plot(df$x, log.lead.conc, xlab="x", ylab="Log lead concentration")
## Produce histograms or the lead concentrations on the original and
## log transformed scales
hist(lead.conc, xlab="Lead concentrations", main="")
hist(log.lead.conc, xlab="Log transformed lead concentration", main="")
## Are there any trends by x and y coordinates?
plot(df$x, lead.conc, xlab="x", ylab="Lead concentration")
plot(df$x, log.lead.conc, xlab="x", ylab="Log lead concentration")
plot(df$y, lead.conc, xlab="y", ylab="Lead concentration")
plot(df$y, log.lead.conc, xlab="y", ylab="Lead concentration")
## Produce a map of the sampling locations
plot(df$x, df$y, xlab="", ylab="", pch=20)
polygon(coastline, border="gray")
## Carry out a t.test using the lead concentrations
t.test(lead.conc, mu=2)
## Carry out a t.test using the log lead concentrations
t.test(log.lead.conc, mu=log(2))
## Read in the coordinates defining the Galicia coastline, converting to the
## same spatial scale as the figure.
coastline <- read.table("/Users/matthewosborne/Documents/Course Work/SpatialStats/galicia_coastline.txt", header=FALSE) / 100000
## Read in the lead values for the year 2000
df <- read.table("/Users/matthewosborne/Documents/Course Work/SpatialStats/galicia_lead_2000.txt", skip=2, header=TRUE)
## Convert to the same spatial scale as the figure
df$x <- df$x / 100000
df$y <- df$y / 100000
## What are the names of the variables?
names(df)
## Extract the lead concentrations from the data frame 'df'
lead.conc <- df$z
lead.conc
lead.conc
lead.conc
## Calculate the log lead concentrations
log.lead.conc <- log(lead.conc)
round(log.lead.conc, 2)
## Produce a panel of plots, 4 rows high, by 2 columns across.
## Adjust the margins slightly for each plot.
par(mfrow=c(4,2), mar=c(4,4,1,1))
## Produce histograms or the lead concentrations on the original and
## log transformed scales
hist(lead.conc, xlab="Lead concentrations", main="")
hist(log.lead.conc, xlab="Log transformed lead concentration", main="")
## Are there any trends by x and y coordinates?
plot(df$x, lead.conc, xlab="x", ylab="Lead concentration")
plot(df$x, log.lead.conc, xlab="x", ylab="Log lead concentration")
plot(df$y, lead.conc, xlab="y", ylab="Lead concentration")
plot(df$y, log.lead.conc, xlab="y", ylab="Lead concentration")
## Produce a map of the sampling locations
plot(df$x, df$y, xlab="", ylab="", pch=20)
polygon(coastline, border="gray")
## Carry out a t.test using the lead concentrations
t.test(lead.conc, mu=2)
## Carry out a t.test using the log lead concentrations
t.test(log.lead.conc, mu=log(2))
? t.test
install.packages(Lahman)
install.packages('Lahman')
library(Lahman)
Teams
tail(Teams,4)
my_teams <- Teams %>% filter(yearID>2000) %>% select(teamID,yearID,lgID,G,W,L,R,RA)
library(dplyr)
my_teams <- Teams %>% filter(yearID>2000) %>% select(teamID,yearID,lgID,G,W,L,R,RA)
lm(data = my_teams,W ~ (R-RA))
library(ggplot)
library(ggplot2)
p <- ggplot(data=my_teams,aes=(x=))
my_teams$RD <- my_teams$R - my_teams$RA
my_teams$RD
p <- ggplot(data=my_teams,aes(x=RD,y=W))
p + geom_point()
setwd("/Users/matthewosborne/Documents/Erdos/PythonCurriculum/Lectures/Regression/")
write.csv(my_teams,'baseball_run_diff.csv')
?write.csv
write.csv(my_teams,'baseball_run_diff.csv',row.names=False)
write.csv(my_teams,'baseball_run_diff.csv',row.names=FALSE)
install.packages("ISLR")
library(ISLR)
data("diamonds")
diamonds
data(Advertising)
data("Carseats")
Carseats
Carseats
write.csv(Carseats,'carseats.csv',row.names=FALSE)
