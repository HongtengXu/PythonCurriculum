\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry} 

\begin{document}
	\noindent
	\textbf{\Large{Daily Lesson Plans}}
	
	\vspace{4mm}
	\noindent
	In this file is outlined the flow of daily lessons for the boot camp. Each day's topics are briefly summarized with bullet points.
	
	
	\vspace{4mm}
	\noindent
	\textit{\large{Day 1: Collecting Data}}
	
	\noindent
	If we are asking students to complete an end to end data science project, they need to be able to find data. This lesson should give them the tools necessary to do that. Here is a brief outline of the flow of the lesson
	\begin{itemize}
		\item Briefly overview various popular online data sites, for example display Kaggle.com, show how you can find data sets, challenges, and kernels.
		\item Introduce the beautifulsoup package.
		\item Go through a couple simple beautiful soup examples.
		\item Explain APIs in a nutshell.
		\item Give one or two examples of python API packages:
			\begin{itemize}
				\item One where no security credentials are needed,
				\item One that requires security credentials.
			\end{itemize}
		\item Common methods of storing data,
			\begin{itemize}
				\item csv.
				\item tab-delimited.
				\item json.
				\item databases.
			\end{itemize}
		\item Potential HW:
			\begin{itemize}
				\item Find an interesting online data set and save it to your laptop.
				\item Use beautifulsoup to scrape an online spreadsheet, or save some txt to file.
				\item If one of the python APIs interested you practice scraping some data with it.
			\end{itemize}
	\end{itemize}
	
	
	
	
	
	
	\vspace{2mm}
	\noindent
	\textit{\large{Day 2: Supervised Learning - Regression 1}}
	
	\noindent
	In this lesson we start supervised learning with regression modeling.
	
	\begin{itemize}
		\item What is supervised learning?
			\begin{itemize}
				\item Using ``labeled'' data to teach a machine to generalize trends in data to ``unlabeled'' data,
				\item Two flavors of problem: Regression and Classification.
			\end{itemize}
		\item What is regression?
			\begin{itemize}
				\item In a machine learning setting, problems where the ``labeled'' data has a continuous measure, for example predicting opening weekend ticket sales.
				\item Be careful not to confuse with a more statistical definition, for example, logistic regression is a statistical regression model, but is most often used as a classification algorithm
			\end{itemize}
		\item The most basic model: Simple Linear Regression
			\begin{itemize}
				\item Introduce model, explain assumptions.
				\item Give example using real data set.
				\item Introduce Training-Test Split
				\item Interpreting results
				\item Model validation for Simple Linear Regression
			\end{itemize}
		\item Potential HW
			\begin{itemize}
				\item Build SLR model with new data
				\item Lay seeds for why you might want ``multiple'' training-test splits, i.e. cross-validation
			\end{itemize}
	\end{itemize}

	\begin{itemize}
		\item More than one predictor: Multiple Linear Regression
		\begin{itemize}
			\item Introduce model, explain assumptions.
			\item Give example, ideally extending example from SLR.
			\item Overfitting example with synthetic data.
			\item Discuss variable selection algorithms.
			\item Additional model validations for MLR.
		\end{itemize}
		\item More than just linear relationships: Polynomial Regression
		\item Additional topics for exploration:
		\begin{itemize}
			\item Handling continuous vs categorical predictors.
			\item Data pre-processing.
			\item cross-validation
		\end{itemize}
		\item Potential HW
		\begin{itemize}
			\item Build the ``best'' regression model possible for a given data set using the techniques covered to this point
		\end{itemize}
	\end{itemize}
	
	
	\vspace{2mm}
	\noindent
	\textit{\large{Day 3: Supervised Learning: Regression 2}}
	
	\noindent
	First complete anything that might be leftover from Day 2.
	
	\noindent
	In regression day 2 we cover two additional regression algorithms, these address one solution to overfitting, regularization.
	\begin{itemize}
		\item Overfitting to the extreme
			\begin{itemize}
				\item Show an example of overfitting with MLR using synthetic random data,
				\item Show an example of overfitting with polynomial regression using synthetic data.
			\end{itemize}
		\item Cost functions
			\begin{itemize}
				\item Remind students of goal in least squares,
				\item If not already defined, define cost/loss functions
			\end{itemize}
		\item Fighting overfitting
			\begin{itemize}
				\item Add penalty term to cost,
				\item Introduce concept of hyperparameter
			\end{itemize}
		\item Ridge Regression
		\item Lasso
		\item Potential HW:
			\begin{itemize}
				\item Wrap up regression section by introducing local regression,
				\item Introduce idea of logistic regression.
			\end{itemize}
	\end{itemize}
	
	\vspace{2mm}
	\noindent
	\textit{\large{Day 4: Supervised Learning: Classification 1}}
	
	\noindent
	We will start off classification with a general outlay of the problem. We will then progress through the general classification workflow with logistic regression.
	
	\noindent
	\begin{itemize}
		\item Explaining the Classification Problem
		\item Logistic Regression
			\begin{itemize}
				\item The model,
				\item General model validation for classification problems,
				\item Precision vs. recall,
				\item Confusion matrix,
				\item ROC curve
			\end{itemize}
		\item Nearest Neighbors
		\item Potential HW
			\begin{itemize}
				\item Build a logistic regression model,
				\item Use CV to tune number of neighbors,
				\item Introduce Naive Bayes.
			\end{itemize}
	\end{itemize}
	
	\vspace{2mm}
	\noindent
	\textit{\large{Day 5: Supervised Learning: Classification 2}}
	
	\noindent
	In today's lesson we learn about tree based methods for classification. We start by building a decision tree model and then branch into random forest models.
	\begin{itemize}
		\item What is the essence of a tree based method?
			\begin{itemize}
				\item We are segmenting the data to make predictions based on some sort of loss optimization criterion.
				\item Called a tree because we can make a tree (in the graph theoretical sense) to represent the various decision rules.
			\end{itemize}
		\item Classification Decision Trees:
			\begin{itemize}
				\item The algorithm,
				\item Give an example,
				\item Advantages and Disadvantages.
			\end{itemize}
		\item Methods of Improving Decision Trees:
			\begin{itemize}
				\item Bagging,
				\item Pasting,
				\item Boosting.
			\end{itemize}
		\item Random Forest Models
			\begin{itemize}
				\item The algorithm,
				\item Extend decision tree example,
				\item Advantages and Disadvantages.
			\end{itemize}
		\item Potential HW:
			\begin{itemize}
				\item Build a decision tree model,
				\item Build a Random Forest model,
				\item Walk through using decision trees for regression tasks.
			\end{itemize}
	\end{itemize}
	
	\vspace{2mm}
	\noindent
	\textit{\large{Day 6: Supervised Learning: Classification 3}}
	
	\noindent
	In the final day of classification we wrap up with support vector machines. We will also discuss voting methods as a way to include all of the classification algorithms we have discussed.
	\begin{itemize}
		\item SVMs:
			\begin{itemize}
				\item Separating Hyperplanes,
				\item The maximal margin classifier,
				\item Support vector classifier,
				\item Support machines
			\end{itemize}
		\item Voting Methods
			\begin{itemize}
				\item Summarize all techniques up to the point,
				\item Explain idea behind the voting method,
				\item Give example.
			\end{itemize}
		\item Potential HW:
			\begin{itemize}
				\item Solve a classification problem using SVMs,
				\item Solve a classification problem using multiple algorithms, compare and contrast them, and then combine them into a single voting algorithm.
			\end{itemize}
	\end{itemize}
	
	\vspace{2mm}
	\noindent
	\textit{\large{Day 7: Unsupervised Learning: Dimensionality Reduction}}
	
	\noindent
	To start off the unsupervised learning portion of the course we discuss the differences between supervised and unsupervised learning. We then present dimensionality reduction techniques.
	\begin{itemize}
		\item Differences between supervised and unsupervised learning.
		\item What is dimensionality reduction? Why do it?
			\begin{itemize}
				\item Reducing the number of variables in our data set,
				\item Memory purposes,
				\item Algorithm speed,
				\item Data Visualization,
				\item Feature Extraction.
			\end{itemize}
		\item Principal Components Analysis (PCA):
			\begin{itemize}
				\item Concept,
				\item The algorithm,
				\item Interpretation of results
				\item Example for visualization,
				\item Example for feature extraction.
			\end{itemize}
		\item Extensions of PCA:
			\begin{itemize}
				\item Multiple correspondence analysis,
				\item Sparse PCA,
				\item Kernel PCA.
			\end{itemize}
		\item A Sampling of manifold techniques:
			\begin{itemize}
				\item Isomap,
				\item t-SNE.
			\end{itemize}
		\item Potential HW:
			\begin{itemize}
				\item Use PCA to extract import features from a dataset,
				\item Dive deeper into kernel or sparse PCA,
				\item Introduce an additional manifold technique.
				\item Work through an anomaly detection example using dimensionality reduction.
			\end{itemize}
	\end{itemize}
	
	\vspace{2mm}
	\noindent
	\textit{\large{Day 8: Unsupervised Learning: Clustering}}
	
	\noindent
	We complete the unsupervised learning section with a discussion on clustering techniques.
	\begin{itemize}
		\item What is clustering?
		\begin{itemize}
			\item Grouping of unlabeled data based on some sort of similarity measure.
			\item Why might we want to do this?
		\end{itemize}
		\item k-means clustering:
		\begin{itemize}
			\item Give algorithm,
			\item How to evaluate results,
			\item Advantages and disadvantages.
		\end{itemize}
		\item Hierarchical clustering:
		\begin{itemize}
			\item The algorithm,
			\item Dendograms,
			\item Evaluating the results,
			\item Advantages and disadvantages.
		\end{itemize}
		\item DBScan:
		\begin{itemize}
			\item The algorithm,
			\item Evaluating the results,
			\item Advantages and disadvantages.
		\end{itemize}
		\item Mixed-membership clustering models:
		\begin{itemize}
			\item Gaussian mixture models.
		\end{itemize}
		\item Potential HW:
		\begin{itemize}
			\item Apply each of the clustering algorithms to different data sets,
			\item Which algorithms perform ``best'' on what kinds of data sets,
			\item Work through a group segmentation example using clustering.
		\end{itemize}
	\end{itemize}
	
	
	\vspace{2mm}
	\noindent
	\textit{\large{Day 9: Neural Networks 1}}
		\begin{itemize}
			\item The Perceptron
				\begin{itemize}
					\item The model,
					\item Simple example of perceptron as binary classifier,
					\item Deficiencies - no XOR.
				\end{itemize}
			\item TBD
		\end{itemize}
	
	\noindent 
	
	
	\vspace{2mm}
	\noindent
	\textit{\large{Day 10: Neural Networks 2}}
		\begin{itemize}
			\item TBD
		\end{itemize}
	
	
	
	\vspace{2mm}
	\noindent
	\textit{\large{Day 11: Time Series Data 1}} 
		\begin{itemize}
			\item Examples of Time Series Data.
			\item Datetime in python.
			\item Issues with applying our prior machine learning model to time series data.
			\item Differences between forecasting and supervised learning.
			\item Simple Forecasting Methods.
			\item Potential Homework:
				\begin{itemize}
					\item Clean and visualize time series data,
					\item Convert time series data between different scales,
					\item Use simple forecasting methods to make forecasts.
				\end{itemize}
		\end{itemize}
	
	\vspace{2mm}
	\noindent
	\textit{\large{Day 12: Time Series Data 2}} 
		\begin{itemize}
			\item Time regression models.
			\item Smoothing methods.
			\item Exponential smoothing.
			\item Potential Homework:
			\begin{itemize}
				\item Return to the time series data from day 11, build a better forecast.
			\end{itemize}
		\end{itemize}
	
	
	\textbf{Note: it is also important for us to cover being able to run code from terminal/command line. So working that in at some point would be good.}
\end{document}